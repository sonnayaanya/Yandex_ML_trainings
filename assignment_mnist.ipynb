{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1: \n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to .\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 9912422/9912422 [00:09<00:00, 1095957.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to .\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 1693021.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\train-labels-idx1-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 1648877/1648877 [00:01<00:00, 1128809.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to .\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 4805885.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to .\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 6')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistclassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(mnistclassifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(28 * 28, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear2_drop = nn.Dropout2d()\n",
    "        self.final = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.relu(self.linear2(x))\n",
    "        x = self.linear2_drop(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mnistclassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1875], Loss: 0.7651\n",
      "Epoch [1/10], Step [200/1875], Loss: 0.7831\n",
      "Epoch [1/10], Step [300/1875], Loss: 0.5873\n",
      "Epoch [1/10], Step [400/1875], Loss: 0.3853\n",
      "Epoch [1/10], Step [500/1875], Loss: 0.4691\n",
      "Epoch [1/10], Step [600/1875], Loss: 0.5347\n",
      "Epoch [1/10], Step [700/1875], Loss: 0.3121\n",
      "Epoch [1/10], Step [800/1875], Loss: 0.5635\n",
      "Epoch [1/10], Step [900/1875], Loss: 0.2361\n",
      "Epoch [1/10], Step [1000/1875], Loss: 0.7626\n",
      "Epoch [1/10], Step [1100/1875], Loss: 0.1490\n",
      "Epoch [1/10], Step [1200/1875], Loss: 0.3222\n",
      "Epoch [1/10], Step [1300/1875], Loss: 0.0871\n",
      "Epoch [1/10], Step [1400/1875], Loss: 0.5678\n",
      "Epoch [1/10], Step [1500/1875], Loss: 0.2677\n",
      "Epoch [1/10], Step [1600/1875], Loss: 0.1287\n",
      "Epoch [1/10], Step [1700/1875], Loss: 0.2111\n",
      "Epoch [1/10], Step [1800/1875], Loss: 0.3029\n",
      "Epoch [2/10], Step [100/1875], Loss: 0.0785\n",
      "Epoch [2/10], Step [200/1875], Loss: 0.0350\n",
      "Epoch [2/10], Step [300/1875], Loss: 0.2469\n",
      "Epoch [2/10], Step [400/1875], Loss: 0.1432\n",
      "Epoch [2/10], Step [500/1875], Loss: 0.3256\n",
      "Epoch [2/10], Step [600/1875], Loss: 0.1530\n",
      "Epoch [2/10], Step [700/1875], Loss: 0.0627\n",
      "Epoch [2/10], Step [800/1875], Loss: 0.1525\n",
      "Epoch [2/10], Step [900/1875], Loss: 0.4857\n",
      "Epoch [2/10], Step [1000/1875], Loss: 0.0162\n",
      "Epoch [2/10], Step [1100/1875], Loss: 0.1441\n",
      "Epoch [2/10], Step [1200/1875], Loss: 0.1444\n",
      "Epoch [2/10], Step [1300/1875], Loss: 0.4318\n",
      "Epoch [2/10], Step [1400/1875], Loss: 0.1778\n",
      "Epoch [2/10], Step [1500/1875], Loss: 0.0634\n",
      "Epoch [2/10], Step [1600/1875], Loss: 0.2288\n",
      "Epoch [2/10], Step [1700/1875], Loss: 0.2806\n",
      "Epoch [2/10], Step [1800/1875], Loss: 0.0244\n",
      "Epoch [3/10], Step [100/1875], Loss: 0.0894\n",
      "Epoch [3/10], Step [200/1875], Loss: 0.2687\n",
      "Epoch [3/10], Step [300/1875], Loss: 0.0416\n",
      "Epoch [3/10], Step [400/1875], Loss: 0.1417\n",
      "Epoch [3/10], Step [500/1875], Loss: 0.0235\n",
      "Epoch [3/10], Step [600/1875], Loss: 0.0324\n",
      "Epoch [3/10], Step [700/1875], Loss: 0.1555\n",
      "Epoch [3/10], Step [800/1875], Loss: 0.0392\n",
      "Epoch [3/10], Step [900/1875], Loss: 0.0833\n",
      "Epoch [3/10], Step [1000/1875], Loss: 0.1818\n",
      "Epoch [3/10], Step [1100/1875], Loss: 0.0315\n",
      "Epoch [3/10], Step [1200/1875], Loss: 0.3042\n",
      "Epoch [3/10], Step [1300/1875], Loss: 0.0501\n",
      "Epoch [3/10], Step [1400/1875], Loss: 0.1247\n",
      "Epoch [3/10], Step [1500/1875], Loss: 0.2574\n",
      "Epoch [3/10], Step [1600/1875], Loss: 0.1839\n",
      "Epoch [3/10], Step [1700/1875], Loss: 0.1815\n",
      "Epoch [3/10], Step [1800/1875], Loss: 0.2742\n",
      "Epoch [4/10], Step [100/1875], Loss: 0.4104\n",
      "Epoch [4/10], Step [200/1875], Loss: 0.1595\n",
      "Epoch [4/10], Step [300/1875], Loss: 0.2179\n",
      "Epoch [4/10], Step [400/1875], Loss: 0.0661\n",
      "Epoch [4/10], Step [500/1875], Loss: 0.0249\n",
      "Epoch [4/10], Step [600/1875], Loss: 0.0442\n",
      "Epoch [4/10], Step [700/1875], Loss: 0.0475\n",
      "Epoch [4/10], Step [800/1875], Loss: 0.0849\n",
      "Epoch [4/10], Step [900/1875], Loss: 0.0051\n",
      "Epoch [4/10], Step [1000/1875], Loss: 0.1262\n",
      "Epoch [4/10], Step [1100/1875], Loss: 0.0626\n",
      "Epoch [4/10], Step [1200/1875], Loss: 0.0466\n",
      "Epoch [4/10], Step [1300/1875], Loss: 0.0451\n",
      "Epoch [4/10], Step [1400/1875], Loss: 0.0205\n",
      "Epoch [4/10], Step [1500/1875], Loss: 0.1575\n",
      "Epoch [4/10], Step [1600/1875], Loss: 0.0543\n",
      "Epoch [4/10], Step [1700/1875], Loss: 0.2482\n",
      "Epoch [4/10], Step [1800/1875], Loss: 0.0186\n",
      "Epoch [5/10], Step [100/1875], Loss: 0.0835\n",
      "Epoch [5/10], Step [200/1875], Loss: 0.0306\n",
      "Epoch [5/10], Step [300/1875], Loss: 0.0429\n",
      "Epoch [5/10], Step [400/1875], Loss: 0.0336\n",
      "Epoch [5/10], Step [500/1875], Loss: 0.1790\n",
      "Epoch [5/10], Step [600/1875], Loss: 0.0120\n",
      "Epoch [5/10], Step [700/1875], Loss: 0.2174\n",
      "Epoch [5/10], Step [800/1875], Loss: 0.0970\n",
      "Epoch [5/10], Step [900/1875], Loss: 0.0180\n",
      "Epoch [5/10], Step [1000/1875], Loss: 0.0777\n",
      "Epoch [5/10], Step [1100/1875], Loss: 0.0803\n",
      "Epoch [5/10], Step [1200/1875], Loss: 0.0855\n",
      "Epoch [5/10], Step [1300/1875], Loss: 0.0221\n",
      "Epoch [5/10], Step [1400/1875], Loss: 0.2789\n",
      "Epoch [5/10], Step [1500/1875], Loss: 0.0191\n",
      "Epoch [5/10], Step [1600/1875], Loss: 0.1492\n",
      "Epoch [5/10], Step [1700/1875], Loss: 0.0079\n",
      "Epoch [5/10], Step [1800/1875], Loss: 0.0494\n",
      "Epoch [6/10], Step [100/1875], Loss: 0.0092\n",
      "Epoch [6/10], Step [200/1875], Loss: 0.0283\n",
      "Epoch [6/10], Step [300/1875], Loss: 0.0159\n",
      "Epoch [6/10], Step [400/1875], Loss: 0.2800\n",
      "Epoch [6/10], Step [500/1875], Loss: 0.0966\n",
      "Epoch [6/10], Step [600/1875], Loss: 0.0724\n",
      "Epoch [6/10], Step [700/1875], Loss: 0.0226\n",
      "Epoch [6/10], Step [800/1875], Loss: 0.1056\n",
      "Epoch [6/10], Step [900/1875], Loss: 0.0070\n",
      "Epoch [6/10], Step [1000/1875], Loss: 0.0974\n",
      "Epoch [6/10], Step [1100/1875], Loss: 0.0103\n",
      "Epoch [6/10], Step [1200/1875], Loss: 0.2130\n",
      "Epoch [6/10], Step [1300/1875], Loss: 0.0315\n",
      "Epoch [6/10], Step [1400/1875], Loss: 0.0147\n",
      "Epoch [6/10], Step [1500/1875], Loss: 0.0291\n",
      "Epoch [6/10], Step [1600/1875], Loss: 0.0134\n",
      "Epoch [6/10], Step [1700/1875], Loss: 0.0301\n",
      "Epoch [6/10], Step [1800/1875], Loss: 0.0334\n",
      "Epoch [7/10], Step [100/1875], Loss: 0.0965\n",
      "Epoch [7/10], Step [200/1875], Loss: 0.0946\n",
      "Epoch [7/10], Step [300/1875], Loss: 0.0178\n",
      "Epoch [7/10], Step [400/1875], Loss: 0.0020\n",
      "Epoch [7/10], Step [500/1875], Loss: 0.0248\n",
      "Epoch [7/10], Step [600/1875], Loss: 0.0150\n",
      "Epoch [7/10], Step [700/1875], Loss: 0.1853\n",
      "Epoch [7/10], Step [800/1875], Loss: 0.0572\n",
      "Epoch [7/10], Step [900/1875], Loss: 0.0675\n",
      "Epoch [7/10], Step [1000/1875], Loss: 0.0389\n",
      "Epoch [7/10], Step [1100/1875], Loss: 0.0498\n",
      "Epoch [7/10], Step [1200/1875], Loss: 0.1685\n",
      "Epoch [7/10], Step [1300/1875], Loss: 0.1484\n",
      "Epoch [7/10], Step [1400/1875], Loss: 0.0081\n",
      "Epoch [7/10], Step [1500/1875], Loss: 0.3651\n",
      "Epoch [7/10], Step [1600/1875], Loss: 0.0492\n",
      "Epoch [7/10], Step [1700/1875], Loss: 0.0242\n",
      "Epoch [7/10], Step [1800/1875], Loss: 0.0775\n",
      "Epoch [8/10], Step [100/1875], Loss: 0.0028\n",
      "Epoch [8/10], Step [200/1875], Loss: 0.0192\n",
      "Epoch [8/10], Step [300/1875], Loss: 0.0145\n",
      "Epoch [8/10], Step [400/1875], Loss: 0.0024\n",
      "Epoch [8/10], Step [500/1875], Loss: 0.0941\n",
      "Epoch [8/10], Step [600/1875], Loss: 0.0692\n",
      "Epoch [8/10], Step [700/1875], Loss: 0.0678\n",
      "Epoch [8/10], Step [800/1875], Loss: 0.0703\n",
      "Epoch [8/10], Step [900/1875], Loss: 0.0359\n",
      "Epoch [8/10], Step [1000/1875], Loss: 0.0049\n",
      "Epoch [8/10], Step [1100/1875], Loss: 0.0454\n",
      "Epoch [8/10], Step [1200/1875], Loss: 0.1644\n",
      "Epoch [8/10], Step [1300/1875], Loss: 0.1465\n",
      "Epoch [8/10], Step [1400/1875], Loss: 0.0149\n",
      "Epoch [8/10], Step [1500/1875], Loss: 0.0346\n",
      "Epoch [8/10], Step [1600/1875], Loss: 0.0414\n",
      "Epoch [8/10], Step [1700/1875], Loss: 0.0231\n",
      "Epoch [8/10], Step [1800/1875], Loss: 0.0451\n",
      "Epoch [9/10], Step [100/1875], Loss: 0.0689\n",
      "Epoch [9/10], Step [200/1875], Loss: 0.2381\n",
      "Epoch [9/10], Step [300/1875], Loss: 0.0323\n",
      "Epoch [9/10], Step [400/1875], Loss: 0.0649\n",
      "Epoch [9/10], Step [500/1875], Loss: 0.0597\n",
      "Epoch [9/10], Step [600/1875], Loss: 0.0316\n",
      "Epoch [9/10], Step [700/1875], Loss: 0.0035\n",
      "Epoch [9/10], Step [800/1875], Loss: 0.0231\n",
      "Epoch [9/10], Step [900/1875], Loss: 0.0013\n",
      "Epoch [9/10], Step [1000/1875], Loss: 0.0283\n",
      "Epoch [9/10], Step [1100/1875], Loss: 0.0021\n",
      "Epoch [9/10], Step [1200/1875], Loss: 0.0027\n",
      "Epoch [9/10], Step [1300/1875], Loss: 0.3572\n",
      "Epoch [9/10], Step [1400/1875], Loss: 0.0050\n",
      "Epoch [9/10], Step [1500/1875], Loss: 0.1559\n",
      "Epoch [9/10], Step [1600/1875], Loss: 0.0208\n",
      "Epoch [9/10], Step [1700/1875], Loss: 0.0098\n",
      "Epoch [9/10], Step [1800/1875], Loss: 0.1173\n",
      "Epoch [10/10], Step [100/1875], Loss: 0.0193\n",
      "Epoch [10/10], Step [200/1875], Loss: 0.0025\n",
      "Epoch [10/10], Step [300/1875], Loss: 0.0619\n",
      "Epoch [10/10], Step [400/1875], Loss: 0.0183\n",
      "Epoch [10/10], Step [500/1875], Loss: 0.0151\n",
      "Epoch [10/10], Step [600/1875], Loss: 0.0177\n",
      "Epoch [10/10], Step [700/1875], Loss: 0.0139\n",
      "Epoch [10/10], Step [800/1875], Loss: 0.0048\n",
      "Epoch [10/10], Step [900/1875], Loss: 0.0155\n",
      "Epoch [10/10], Step [1000/1875], Loss: 0.2052\n",
      "Epoch [10/10], Step [1100/1875], Loss: 0.0687\n",
      "Epoch [10/10], Step [1200/1875], Loss: 0.0380\n",
      "Epoch [10/10], Step [1300/1875], Loss: 0.1410\n",
      "Epoch [10/10], Step [1400/1875], Loss: 0.0653\n",
      "Epoch [10/10], Step [1500/1875], Loss: 0.0185\n",
      "Epoch [10/10], Step [1600/1875], Loss: 0.0190\n",
      "Epoch [10/10], Step [1700/1875], Loss: 0.0086\n",
      "Epoch [10/10], Step [1800/1875], Loss: 0.0646\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "total_step = len(train_data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_data_loader):\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.99405\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.9782\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_hw07.npy`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
    "}\n",
    "\n",
    "np.save('submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
